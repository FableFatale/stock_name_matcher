#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è‚¡ç¥¨åŒ¹é…æ€§èƒ½ä¼˜åŒ–å™¨
æä¾›æ‰¹é‡å¤„ç†ã€ç¼“å­˜ã€å¹¶è¡Œå¤„ç†ç­‰ä¼˜åŒ–åŠŸèƒ½
"""

import os
import sys
import time
import logging
import pandas as pd
import numpy as np
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Dict, Tuple
import threading
from functools import lru_cache

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class PerformanceOptimizer:
    """æ€§èƒ½ä¼˜åŒ–å™¨"""
    
    def __init__(self, stock_matcher):
        self.matcher = stock_matcher
        self.cache = {}
        self.cache_lock = threading.Lock()
        self.batch_size = 100
        self.max_workers = 4
        
    def optimize_stock_matching(self, input_df: pd.DataFrame, enable_cross_validation: bool = False) -> List[Dict]:
        """
        ä¼˜åŒ–çš„è‚¡ç¥¨åŒ¹é…å¤„ç†
        
        Args:
            input_df: è¾“å…¥æ•°æ®æ¡†
            enable_cross_validation: æ˜¯å¦å¯ç”¨äº¤å‰éªŒè¯
            
        Returns:
            List[Dict]: åŒ¹é…ç»“æœåˆ—è¡¨
        """
        logger.info(f"ğŸš€ å¼€å§‹ä¼˜åŒ–å¤„ç† {len(input_df)} æ¡è®°å½•...")
        start_time = time.time()
        
        # 1. é¢„å¤„ç†å’Œå»é‡
        processed_data = self._preprocess_data(input_df)
        
        # 2. æ‰¹é‡å¤„ç†
        if len(processed_data) > self.batch_size:
            results = self._batch_process(processed_data, enable_cross_validation)
        else:
            results = self._parallel_process(processed_data, enable_cross_validation)
        
        # 3. åå¤„ç†
        final_results = self._postprocess_results(results, input_df)
        
        end_time = time.time()
        logger.info(f"âœ… ä¼˜åŒ–å¤„ç†å®Œæˆï¼Œè€—æ—¶: {end_time - start_time:.2f}ç§’")
        logger.info(f"ğŸ“Š å¹³å‡æ¯æ¡è®°å½•è€—æ—¶: {(end_time - start_time) / len(input_df):.3f}ç§’")
        
        return final_results
    
    def _preprocess_data(self, input_df: pd.DataFrame) -> List[Tuple[int, str, float]]:
        """é¢„å¤„ç†æ•°æ®ï¼Œå»é‡å’Œæ ‡å‡†åŒ–"""
        logger.info("ğŸ“‹ é¢„å¤„ç†æ•°æ®...")
        
        processed_data = []
        seen_codes = set()
        
        for idx, row in input_df.iterrows():
            code = str(row['è‚¡ç¥¨ä»£ç ']).strip()
            price = row.get('å‚è€ƒä»·æ ¼', None)
            
            # æ ‡å‡†åŒ–ä»£ç 
            normalized_code = self.matcher._normalize_stock_code(code)
            
            # å»é‡å¤„ç†
            cache_key = (normalized_code, price)
            if cache_key not in seen_codes:
                seen_codes.add(cache_key)
                processed_data.append((idx, normalized_code, price))
            else:
                # å¦‚æœæ˜¯é‡å¤çš„ï¼Œç›´æ¥ä»ç¼“å­˜è·å–
                processed_data.append((idx, normalized_code, price))
        
        logger.info(f"ğŸ“Š é¢„å¤„ç†å®Œæˆï¼Œå»é‡å: {len(set(seen_codes))} ä¸ªå”¯ä¸€ä»£ç ")
        return processed_data
    
    def _parallel_process(self, data: List[Tuple[int, str, float]], enable_cross_validation: bool) -> List[Dict]:
        """å¹¶è¡Œå¤„ç†å°æ‰¹é‡æ•°æ®"""
        logger.info(f"âš¡ å¹¶è¡Œå¤„ç† {len(data)} æ¡è®°å½•...")
        
        results = [None] * len(data)
        
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # æäº¤ä»»åŠ¡
            future_to_index = {}
            for i, (idx, code, price) in enumerate(data):
                future = executor.submit(self._process_single_with_cache, code, price, enable_cross_validation)
                future_to_index[future] = (i, idx)
            
            # æ”¶é›†ç»“æœ
            completed = 0
            for future in as_completed(future_to_index):
                result_idx, original_idx = future_to_index[future]
                try:
                    result = future.result()
                    result['åŸå§‹ç´¢å¼•'] = original_idx
                    results[result_idx] = result
                    completed += 1
                    
                    if completed % 50 == 0:
                        logger.info(f"ğŸ“ˆ å·²å®Œæˆ: {completed}/{len(data)}")
                        
                except Exception as e:
                    logger.error(f"å¤„ç†å¤±è´¥: {e}")
                    results[result_idx] = self._create_error_result(data[result_idx][1], data[result_idx][2], str(e))
        
        return [r for r in results if r is not None]
    
    def _batch_process(self, data: List[Tuple[int, str, float]], enable_cross_validation: bool) -> List[Dict]:
        """æ‰¹é‡å¤„ç†å¤§é‡æ•°æ®"""
        logger.info(f"ğŸ“¦ æ‰¹é‡å¤„ç† {len(data)} æ¡è®°å½•...")
        
        all_results = []
        total_batches = (len(data) + self.batch_size - 1) // self.batch_size
        
        for batch_idx in range(0, len(data), self.batch_size):
            batch_data = data[batch_idx:batch_idx + self.batch_size]
            batch_num = batch_idx // self.batch_size + 1
            
            logger.info(f"ğŸ”„ å¤„ç†æ‰¹æ¬¡ {batch_num}/{total_batches} ({len(batch_data)} æ¡è®°å½•)")
            
            # å¹¶è¡Œå¤„ç†å½“å‰æ‰¹æ¬¡
            batch_results = self._parallel_process(batch_data, enable_cross_validation)
            all_results.extend(batch_results)
            
            # æ‰¹æ¬¡é—´çŸ­æš‚ä¼‘æ¯ï¼Œé¿å…APIé™åˆ¶
            if batch_num < total_batches:
                time.sleep(0.5)
        
        return all_results
    
    def _process_single_with_cache(self, code: str, price: float, enable_cross_validation: bool) -> Dict:
        """å¸¦ç¼“å­˜çš„å•ä¸ªè‚¡ç¥¨å¤„ç†"""
        cache_key = (code, price, enable_cross_validation)
        
        # æ£€æŸ¥ç¼“å­˜
        with self.cache_lock:
            if cache_key in self.cache:
                return self.cache[cache_key].copy()
        
        # å¤„ç†è‚¡ç¥¨
        try:
            result = self.matcher.match_stock_code(code, price, enable_cross_validation)
            
            # ç¼“å­˜ç»“æœ
            with self.cache_lock:
                self.cache[cache_key] = result.copy()
            
            return result
            
        except Exception as e:
            logger.error(f"å¤„ç†è‚¡ç¥¨ {code} å¤±è´¥: {e}")
            return self._create_error_result(code, price, str(e))
    
    def _create_error_result(self, code: str, price: float, error_msg: str) -> Dict:
        """åˆ›å»ºé”™è¯¯ç»“æœ"""
        return {
            'åŸå§‹ä»£ç ': code,
            'å‚è€ƒä»·æ ¼': price,
            'åŒ¹é…çŠ¶æ€': 'å¤„ç†å¤±è´¥',
            'æ ‡å‡†åŒ–ä»£ç ': code,
            'è‚¡ç¥¨åç§°': '',
            'å½“å‰ä»·æ ¼': '',
            'ä»·æ ¼å·®å¼‚': '',
            'åŒ¹é…ç±»å‹': f'é”™è¯¯: {error_msg}'
        }
    
    def _postprocess_results(self, results: List[Dict], original_df: pd.DataFrame) -> List[Dict]:
        """åå¤„ç†ç»“æœï¼Œç¡®ä¿é¡ºåºæ­£ç¡®"""
        logger.info("ğŸ”§ åå¤„ç†ç»“æœ...")
        
        # åˆ›å»ºç»“æœæ˜ å°„
        result_map = {}
        for result in results:
            if 'åŸå§‹ç´¢å¼•' in result:
                result_map[result['åŸå§‹ç´¢å¼•']] = result
                del result['åŸå§‹ç´¢å¼•']  # ç§»é™¤ä¸´æ—¶å­—æ®µ
        
        # æŒ‰åŸå§‹é¡ºåºé‡æ–°æ’åˆ—
        final_results = []
        for idx in range(len(original_df)):
            if idx in result_map:
                final_results.append(result_map[idx])
            else:
                # å¦‚æœæ²¡æœ‰ç»“æœï¼Œåˆ›å»ºé»˜è®¤ç»“æœ
                row = original_df.iloc[idx]
                final_results.append(self._create_error_result(
                    str(row['è‚¡ç¥¨ä»£ç ']), 
                    row.get('å‚è€ƒä»·æ ¼', None), 
                    'æœªå¤„ç†'
                ))
        
        return final_results
    
    def get_cache_stats(self) -> Dict:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        with self.cache_lock:
            return {
                'cache_size': len(self.cache),
                'cache_hit_potential': len(self.cache) > 0
            }
    
    def clear_cache(self):
        """æ¸…ç©ºç¼“å­˜"""
        with self.cache_lock:
            self.cache.clear()
            logger.info("ğŸ—‘ï¸ ç¼“å­˜å·²æ¸…ç©º")


def apply_performance_optimization():
    """åº”ç”¨æ€§èƒ½ä¼˜åŒ–åˆ°ç°æœ‰çš„è‚¡ç¥¨åŒ¹é…å™¨"""
    logger.info("ğŸ”§ åº”ç”¨æ€§èƒ½ä¼˜åŒ–...")
    
    # è¿™ä¸ªå‡½æ•°å°†è¢«ç”¨æ¥ä¿®æ”¹ç°æœ‰çš„ stock_name_matcher.py
    optimization_code = '''
    # æ€§èƒ½ä¼˜åŒ–ä»£ç å°†è¢«æ³¨å…¥åˆ° StockNameMatcher ç±»ä¸­
    def process_stock_codes_optimized(self, file_path: str, output_path: str = None,
                                    code_column: str = None, price_column: str = None,
                                    enable_cross_validation: bool = False) -> str:
        """
        ä¼˜åŒ–ç‰ˆæœ¬çš„è‚¡ç¥¨ä»£ç å¤„ç†
        """
        from performance_optimizer import PerformanceOptimizer
        
        # è¯»å–æ–‡ä»¶
        input_df = self.read_excel_file(file_path, code_column=code_column, price_column=price_column)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰è‚¡ç¥¨ä»£ç åˆ—
        if 'è‚¡ç¥¨ä»£ç ' not in input_df.columns or input_df['è‚¡ç¥¨ä»£ç '].isna().all():
            raise ValueError("æœªæ‰¾åˆ°æœ‰æ•ˆçš„è‚¡ç¥¨ä»£ç åˆ—ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æ ¼å¼æˆ–æŒ‡å®šæ­£ç¡®çš„åˆ—å")
        
        # ä½¿ç”¨æ€§èƒ½ä¼˜åŒ–å™¨
        optimizer = PerformanceOptimizer(self)
        results = optimizer.optimize_stock_matching(input_df, enable_cross_validation)
        
        # ä¿å­˜ç»“æœ
        result_df = pd.DataFrame(results)
        
        # ç¡®ä¿è‚¡ç¥¨ä»£ç ç›¸å…³åˆ—ä¿æŒå­—ç¬¦ä¸²æ ¼å¼ï¼Œä¿ç•™å‰å¯¼é›¶
        code_columns = ['æ ‡å‡†åŒ–ä»£ç ', 'è‚¡ç¥¨ä»£ç ']
        for col in code_columns:
            if col in result_df.columns:
                result_df[col] = result_df[col].astype(str)
        
        if output_path is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_path = f"stock_code_completion_{timestamp}.csv"
        
        # ä¿å­˜CSVæ—¶æŒ‡å®šæ•°æ®ç±»å‹ï¼Œç¡®ä¿è‚¡ç¥¨ä»£ç åˆ—ä¿æŒå­—ç¬¦ä¸²æ ¼å¼
        result_df.to_csv(output_path, index=False, encoding='utf-8-sig')
        
        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        total_count = len(result_df)
        success_count = len(result_df[result_df['åŒ¹é…çŠ¶æ€'].str.contains('åŒ¹é…æˆåŠŸ', na=False)])
        
        logger.info(f"ä¼˜åŒ–å¤„ç†ç»Ÿè®¡:")
        logger.info(f"  æ€»æ•°: {total_count}")
        logger.info(f"  æˆåŠŸè¡¥å…¨: {success_count} ({success_count/total_count*100:.1f}%)")
        logger.info(f"  ç¼“å­˜ç»Ÿè®¡: {optimizer.get_cache_stats()}")
        
        return output_path
    '''
    
    return optimization_code


if __name__ == '__main__':
    print("ğŸš€ è‚¡ç¥¨åŒ¹é…æ€§èƒ½ä¼˜åŒ–å™¨")
    print("ğŸ“‹ ä¸»è¦ä¼˜åŒ–åŠŸèƒ½:")
    print("  - å¹¶è¡Œå¤„ç†")
    print("  - æ™ºèƒ½ç¼“å­˜")
    print("  - æ‰¹é‡å¤„ç†")
    print("  - å»é‡ä¼˜åŒ–")
    print("  - å‡å°‘APIè°ƒç”¨å»¶è¿Ÿ")
