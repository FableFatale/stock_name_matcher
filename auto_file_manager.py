#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è‡ªåŠ¨æ–‡ä»¶ç®¡ç†å™¨
è‡ªåŠ¨æ£€æµ‹å’Œç®¡ç†è‚¡ç¥¨æ•°æ®æ–‡ä»¶ï¼Œç¡®ä¿ç³»ç»Ÿå§‹ç»ˆä½¿ç”¨æœ€æ–°çš„æ•°æ®
"""

import os
import sys
import time
import logging
import shutil
from datetime import datetime
from pathlib import Path
import pandas as pd

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class AutoFileManager:
    """è‡ªåŠ¨æ–‡ä»¶ç®¡ç†å™¨"""
    
    def __init__(self, watch_directory: str = "stock_name_list"):
        self.watch_directory = watch_directory
        self.data_directory = "data"
        self.backup_directory = "backup"

        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(self.watch_directory, exist_ok=True)
        os.makedirs(self.data_directory, exist_ok=True)
        os.makedirs(self.backup_directory, exist_ok=True)

        # åˆ›å»ºè¯´æ˜æ–‡ä»¶
        self._create_readme_file()
        
        # æ”¯æŒçš„æ–‡ä»¶æ¨¡å¼ - ç®€åŒ–ä¸ºæ”¯æŒæ‰€æœ‰CSVæ–‡ä»¶
        self.supported_patterns = [
            # ä»»ä½•CSVæ–‡ä»¶éƒ½å¯ä»¥ï¼Œè®©éªŒè¯å‡½æ•°æ¥åˆ¤æ–­å†…å®¹æ˜¯å¦æ­£ç¡®
        ]
    
    def is_stock_data_file(self, filename: str) -> bool:
        """
        åˆ¤æ–­æ–‡ä»¶æ˜¯å¦ä¸ºè‚¡ç¥¨æ•°æ®æ–‡ä»¶
        åœ¨ stock_name_list ç›®å½•ä¸­ï¼Œæ‰€æœ‰CSVæ–‡ä»¶éƒ½è¢«è®¤ä¸ºæ˜¯æ½œåœ¨çš„è‚¡ç¥¨æ•°æ®æ–‡ä»¶
        å…·ä½“éªŒè¯é€šè¿‡ validate_stock_file å‡½æ•°è¿›è¡Œ
        """
        return filename.lower().endswith('.csv')

    def _select_best_data_file(self, data_files: list) -> str:
        """
        ä»å€™é€‰æ–‡ä»¶ä¸­é€‰æ‹©æœ€ä½³çš„è‚¡ç¥¨æ•°æ®æ–‡ä»¶
        ä¼˜å…ˆçº§ï¼š
        1. åŒ…å«'latest'çš„æ–‡ä»¶
        2. æ–‡ä»¶ååŒ…å«æœ€æ–°æ—¥æœŸçš„æ–‡ä»¶
        3. ä¿®æ”¹æ—¶é—´æœ€æ–°çš„æ–‡ä»¶
        4. æ–‡ä»¶å¤§å°æœ€å¤§çš„æ–‡ä»¶
        """
        if not data_files:
            return None

        # 1. ä¼˜å…ˆä½¿ç”¨åŒ…å«'latest'çš„æ–‡ä»¶
        latest_files = [f for f in data_files if "latest" in os.path.basename(f).lower()]
        if latest_files:
            # å¦‚æœæœ‰å¤šä¸ªlatestæ–‡ä»¶ï¼Œé€‰æ‹©ä¿®æ”¹æ—¶é—´æœ€æ–°çš„
            best_file = max(latest_files, key=os.path.getmtime)
            logger.info(f"é€‰æ‹©latestæ–‡ä»¶: {best_file}")
            return best_file

        # 2. å°è¯•ä»æ–‡ä»¶åä¸­æå–æ—¥æœŸï¼Œé€‰æ‹©æœ€æ–°çš„
        import re
        dated_files = []
        for filepath in data_files:
            filename = os.path.basename(filepath)
            # åŒ¹é…æ—¥æœŸæ ¼å¼ï¼šYYYYMMDD æˆ– YYYY-MM-DD æˆ– YYYY_MM_DD
            date_match = re.search(r'(\d{4})[_-]?(\d{2})[_-]?(\d{2})', filename)
            if date_match:
                date_str = ''.join(date_match.groups())
                try:
                    from datetime import datetime
                    date_obj = datetime.strptime(date_str, '%Y%m%d')
                    dated_files.append((filepath, date_obj))
                except:
                    pass

        if dated_files:
            # æŒ‰æ—¥æœŸæ’åºï¼Œé€‰æ‹©æœ€æ–°çš„
            best_file = max(dated_files, key=lambda x: x[1])[0]
            logger.info(f"é€‰æ‹©æœ€æ–°æ—¥æœŸæ–‡ä»¶: {best_file}")
            return best_file

        # 3. æŒ‰ä¿®æ”¹æ—¶é—´é€‰æ‹©æœ€æ–°çš„æ–‡ä»¶
        try:
            best_file = max(data_files, key=os.path.getmtime)
            logger.info(f"é€‰æ‹©ä¿®æ”¹æ—¶é—´æœ€æ–°æ–‡ä»¶: {best_file}")
            return best_file
        except:
            # 4. å¦‚æœè·å–ä¿®æ”¹æ—¶é—´å¤±è´¥ï¼Œé€‰æ‹©æ–‡ä»¶å¤§å°æœ€å¤§çš„
            try:
                best_file = max(data_files, key=os.path.getsize)
                logger.info(f"é€‰æ‹©æ–‡ä»¶å¤§å°æœ€å¤§æ–‡ä»¶: {best_file}")
                return best_file
            except:
                # 5. æœ€åçš„å¤‡é€‰æ–¹æ¡ˆï¼Œé€‰æ‹©ç¬¬ä¸€ä¸ªæ–‡ä»¶
                logger.info(f"ä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨æ–‡ä»¶: {data_files[0]}")
                return data_files[0]

    def _create_readme_file(self):
        """åœ¨ç›‘æ§ç›®å½•åˆ›å»ºè¯´æ˜æ–‡ä»¶"""
        readme_path = os.path.join(self.watch_directory, "README.md")

        if not os.path.exists(readme_path):
            readme_content = """# è‚¡ç¥¨åç§°åˆ—è¡¨ç›®å½•

## ğŸ“ ç›®å½•è¯´æ˜
è¿™ä¸ªç›®å½•ä¸“é—¨ç”¨äºå­˜æ”¾è‚¡ç¥¨ä»£ç å’Œåç§°çš„CSVæ–‡ä»¶ã€‚ç³»ç»Ÿä¼šè‡ªåŠ¨æ‰«ææ­¤ç›®å½•ï¼Œæ‰¾åˆ°æœ€æ–°çš„è‚¡ç¥¨æ•°æ®æ–‡ä»¶å¹¶ä½¿ç”¨ã€‚

## ğŸ“‹ æ–‡ä»¶æ ¼å¼è¦æ±‚
- **CSVæ–‡ä»¶** (å¿…é¡»æ˜¯UTF-8ç¼–ç )
- **å¿…é¡»åŒ…å«çš„åˆ—**ï¼š
  - è‚¡ç¥¨ä»£ç åˆ—ï¼š`code` æˆ– `ä»£ç ` æˆ– `è‚¡ç¥¨ä»£ç `
  - è‚¡ç¥¨åç§°åˆ—ï¼š`name` æˆ– `åç§°` æˆ– `è‚¡ç¥¨åç§°`

## ğŸ“ æ–‡ä»¶å‘½å
- æ”¯æŒä»»ä½• `.csv` æ–‡ä»¶å
- ç³»ç»Ÿä¼šè‡ªåŠ¨é€‰æ‹©æœ€æ–°çš„æ–‡ä»¶ï¼ˆæŒ‰ä¿®æ”¹æ—¶é—´æˆ–æ–‡ä»¶åä¸­çš„æ—¥æœŸï¼‰
- å»ºè®®ä½¿ç”¨åŒ…å«æ—¥æœŸçš„æ–‡ä»¶åï¼Œå¦‚ï¼š`stocks_20250620.csv`

## ğŸš€ ä½¿ç”¨æ–¹æ³•
1. å°†æ–°çš„è‚¡ç¥¨æ•°æ®CSVæ–‡ä»¶æ”¾å…¥æ­¤ç›®å½•
2. ç³»ç»Ÿä¼šè‡ªåŠ¨æ£€æµ‹å¹¶ä½¿ç”¨æœ€æ–°çš„æ–‡ä»¶
3. å¯ä»¥é€šè¿‡Webç•Œé¢æ‰‹åŠ¨è§¦å‘æ›´æ–°
4. æ—§æ–‡ä»¶ä¼šè‡ªåŠ¨å¤‡ä»½åˆ° `backup` ç›®å½•

## ğŸ“Š æ–‡ä»¶ç¤ºä¾‹æ ¼å¼
```csv
code,name
000001,å¹³å®‰é“¶è¡Œ
000002,ä¸‡ç§‘A
600000,æµ¦å‘é“¶è¡Œ
600036,æ‹›å•†é“¶è¡Œ
```

## âš ï¸ æ³¨æ„äº‹é¡¹
- ç¡®ä¿CSVæ–‡ä»¶ç¼–ç ä¸ºUTF-8
- è‚¡ç¥¨ä»£ç åº”ä¸º6ä½æ•°å­—
- æ–‡ä»¶å¤§å°å»ºè®®ä¸è¶…è¿‡50MB
- ç³»ç»Ÿä¼šè‡ªåŠ¨å¤‡ä»½è¢«æ›¿æ¢çš„æ—§æ–‡ä»¶

## ğŸ“ æŠ€æœ¯æ”¯æŒ
å¦‚æœ‰é—®é¢˜ï¼Œè¯·æ£€æŸ¥æ—¥å¿—æ–‡ä»¶æˆ–è”ç³»æŠ€æœ¯æ”¯æŒã€‚
"""

            try:
                with open(readme_path, 'w', encoding='utf-8') as f:
                    f.write(readme_content)
                logger.info(f"å·²åˆ›å»ºè¯´æ˜æ–‡ä»¶: {readme_path}")
            except Exception as e:
                logger.error(f"åˆ›å»ºè¯´æ˜æ–‡ä»¶å¤±è´¥: {e}")

    def scan_for_new_files(self) -> list:
        """æ‰«æè‚¡ç¥¨æ•°æ®æ–‡ä»¶ï¼Œè¿”å›æœ€æ–°çš„æ–‡ä»¶"""
        try:
            all_files = []

            # è·å–æ‰€æœ‰CSVæ–‡ä»¶
            for filename in os.listdir(self.watch_directory):
                if self.is_stock_data_file(filename):
                    filepath = os.path.join(self.watch_directory, filename)
                    all_files.append(filepath)

            if not all_files:
                return []

            # é€‰æ‹©æœ€æ–°çš„æ–‡ä»¶
            latest_file = self._select_best_data_file(all_files)

            if latest_file:
                # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°ï¼ˆæ–‡ä»¶æ˜¯å¦æ¯”å½“å‰ä½¿ç”¨çš„æ›´æ–°ï¼‰
                current_latest = os.path.join(self.data_directory, 'stock_list_latest.csv')
                if not os.path.exists(current_latest):
                    logger.info(f"æœªæ‰¾åˆ°å½“å‰latestæ–‡ä»¶ï¼Œå°†å¤„ç†: {latest_file}")
                    return [latest_file]
                else:
                    # æ¯”è¾ƒä¿®æ”¹æ—¶é—´
                    latest_mtime = os.path.getmtime(latest_file)
                    current_mtime = os.path.getmtime(current_latest)

                    logger.info(f"æ–‡ä»¶æ—¶é—´æ¯”è¾ƒ: {latest_file} ({latest_mtime}) vs current ({current_mtime})")

                    if latest_mtime > current_mtime:
                        logger.info(f"å‘ç°æ›´æ–°çš„æ–‡ä»¶: {latest_file}")
                        return [latest_file]
                    else:
                        logger.info(f"å½“å‰æ–‡ä»¶å·²æ˜¯æœ€æ–°: {current_latest}")

                        # é¢å¤–æ£€æŸ¥ï¼šå¦‚æœæœ‰å¤šä¸ªæ–‡ä»¶ï¼Œå¯èƒ½æœ‰æ–°æ–‡ä»¶
                        if len(all_files) > 1:
                            # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œæ£€æŸ¥æ˜¯å¦æœ‰æ¯”å½“å‰latestæ›´æ–°çš„æ–‡ä»¶
                            sorted_files = sorted(all_files, key=os.path.getmtime, reverse=True)
                            newest_file = sorted_files[0]
                            if os.path.getmtime(newest_file) > current_mtime:
                                logger.info(f"å‘ç°æœ€æ–°æ–‡ä»¶: {newest_file}")
                                return [newest_file]

            return []

        except Exception as e:
            logger.error(f"æ‰«ææ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return []
    
    def validate_stock_file(self, filepath: str) -> dict:
        """éªŒè¯è‚¡ç¥¨æ•°æ®æ–‡ä»¶çš„æœ‰æ•ˆæ€§"""
        result = {
            'valid': False,
            'error': None,
            'info': {}
        }
        
        try:
            # æ£€æŸ¥æ–‡ä»¶å¤§å°
            file_size = os.path.getsize(filepath)
            if file_size == 0:
                result['error'] = "æ–‡ä»¶ä¸ºç©º"
                return result
            
            # å°è¯•è¯»å–æ–‡ä»¶
            df = pd.read_csv(filepath, nrows=10)  # åªè¯»å–å‰10è¡Œè¿›è¡ŒéªŒè¯
            
            # æ£€æŸ¥åˆ—å
            columns = df.columns.tolist()
            has_code = any(col.lower() in ['code', 'ä»£ç ', 'è‚¡ç¥¨ä»£ç '] for col in columns)
            has_name = any(col.lower() in ['name', 'åç§°', 'è‚¡ç¥¨åç§°'] for col in columns)
            
            if not (has_code and has_name):
                result['error'] = f"æ–‡ä»¶ç¼ºå°‘å¿…è¦çš„åˆ—ï¼ˆä»£ç /åç§°ï¼‰ï¼Œå½“å‰åˆ—: {columns}"
                return result
            
            # æ£€æŸ¥æ•°æ®è¡Œæ•°
            total_rows = len(pd.read_csv(filepath))
            if total_rows < 10:
                result['error'] = f"æ•°æ®è¡Œæ•°å¤ªå°‘: {total_rows} è¡Œ"
                return result
            
            result['valid'] = True
            result['info'] = {
                'columns': columns,
                'total_rows': total_rows,
                'file_size': file_size,
                'file_size_mb': round(file_size / (1024 * 1024), 2)
            }
            
            return result
            
        except Exception as e:
            result['error'] = f"è¯»å–æ–‡ä»¶å¤±è´¥: {str(e)}"
            return result
    
    def backup_old_file(self, filename: str) -> bool:
        """å¤‡ä»½æ—§æ–‡ä»¶"""
        try:
            data_filepath = os.path.join(self.data_directory, filename)
            if os.path.exists(data_filepath):
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                backup_filename = f"{os.path.splitext(filename)[0]}_backup_{timestamp}.csv"
                backup_filepath = os.path.join(self.backup_directory, backup_filename)
                
                shutil.copy2(data_filepath, backup_filepath)
                logger.info(f"å·²å¤‡ä»½æ—§æ–‡ä»¶: {backup_filepath}")
                return True
            
            return True  # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œä¹Ÿç®—æˆåŠŸ
            
        except Exception as e:
            logger.error(f"å¤‡ä»½æ–‡ä»¶å¤±è´¥: {e}")
            return False
    
    def install_new_file(self, source_filepath: str) -> bool:
        """å®‰è£…æ–°çš„è‚¡ç¥¨æ•°æ®æ–‡ä»¶"""
        try:
            filename = os.path.basename(source_filepath)
            
            # éªŒè¯æ–‡ä»¶
            validation = self.validate_stock_file(source_filepath)
            if not validation['valid']:
                logger.error(f"æ–‡ä»¶éªŒè¯å¤±è´¥: {validation['error']}")
                return False
            
            logger.info(f"æ–‡ä»¶éªŒè¯é€šè¿‡: {validation['info']}")
            
            # å¤‡ä»½æ—§æ–‡ä»¶
            if not self.backup_old_file(filename):
                logger.warning("å¤‡ä»½æ—§æ–‡ä»¶å¤±è´¥ï¼Œä½†ç»§ç»­å®‰è£…æ–°æ–‡ä»¶")
            
            # å¤åˆ¶æ–°æ–‡ä»¶åˆ°dataç›®å½•
            dest_filepath = os.path.join(self.data_directory, filename)
            shutil.copy2(source_filepath, dest_filepath)

            logger.info(f"æ–°æ–‡ä»¶å·²å®‰è£…: {dest_filepath}")

            # å¦‚æœæ–‡ä»¶åä¸åŒ…å«latestï¼Œåˆ›å»ºä¸€ä¸ªlatesté“¾æ¥
            if 'latest' not in filename.lower():
                self.create_latest_link(filename)

            # ä¸ç§»åŠ¨åŸæ–‡ä»¶ï¼Œä¿ç•™åœ¨stock_name_listç›®å½•ä¸­
            logger.info(f"æ–‡ä»¶ä¿ç•™åœ¨åŸç›®å½•: {source_filepath}")

            return True
            
        except Exception as e:
            logger.error(f"å®‰è£…æ–‡ä»¶å¤±è´¥: {e}")
            return False
    
    def create_latest_link(self, filename: str):
        """åˆ›å»ºlatestæ–‡ä»¶é“¾æ¥"""
        try:
            # ç¡®å®šlatestæ–‡ä»¶å
            base_name = os.path.splitext(filename)[0]
            if base_name.startswith('all_stocks_'):
                latest_filename = 'stock_list_latest.csv'
            else:
                latest_filename = 'stock_list_latest.csv'
            
            source_filepath = os.path.join(self.data_directory, filename)
            latest_filepath = os.path.join(self.data_directory, latest_filename)
            
            # å¦‚æœlatestæ–‡ä»¶å·²å­˜åœ¨ï¼Œå…ˆå¤‡ä»½
            if os.path.exists(latest_filepath):
                self.backup_old_file(latest_filename)
            
            # å¤åˆ¶æ–‡ä»¶ï¼ˆè€Œä¸æ˜¯åˆ›å»ºé“¾æ¥ï¼Œå› ä¸ºWindowsé“¾æ¥å¯èƒ½æœ‰æƒé™é—®é¢˜ï¼‰
            shutil.copy2(source_filepath, latest_filepath)
            logger.info(f"å·²åˆ›å»ºlatestæ–‡ä»¶: {latest_filepath}")
            
        except Exception as e:
            logger.error(f"åˆ›å»ºlatestæ–‡ä»¶å¤±è´¥: {e}")

    def auto_update(self) -> dict:
        """è‡ªåŠ¨æ›´æ–°è‚¡ç¥¨æ•°æ®æ–‡ä»¶"""
        result = {
            'updated': False,
            'new_files': [],
            'errors': []
        }
        
        logger.info("ğŸ” å¼€å§‹è‡ªåŠ¨æ£€æŸ¥æ–°çš„è‚¡ç¥¨æ•°æ®æ–‡ä»¶...")
        
        # æ‰«ææ–°æ–‡ä»¶
        new_files = self.scan_for_new_files()
        
        if not new_files:
            logger.info("âœ… æ²¡æœ‰å‘ç°æ–°çš„è‚¡ç¥¨æ•°æ®æ–‡ä»¶")
            return result
        
        logger.info(f"ğŸ“ å‘ç° {len(new_files)} ä¸ªæ–°æ–‡ä»¶: {[os.path.basename(f) for f in new_files]}")
        
        # å¤„ç†æ¯ä¸ªæ–°æ–‡ä»¶
        for filepath in new_files:
            filename = os.path.basename(filepath)
            logger.info(f"ğŸ“¥ å¤„ç†æ–‡ä»¶: {filename}")
            
            if self.install_new_file(filepath):
                result['new_files'].append(filename)
                result['updated'] = True
                logger.info(f"âœ… æ–‡ä»¶å®‰è£…æˆåŠŸ: {filename}")
            else:
                error_msg = f"æ–‡ä»¶å®‰è£…å¤±è´¥: {filename}"
                result['errors'].append(error_msg)
                logger.error(f"âŒ {error_msg}")
        
        if result['updated']:
            logger.info(f"ğŸ‰ è‡ªåŠ¨æ›´æ–°å®Œæˆï¼ŒæˆåŠŸå®‰è£… {len(result['new_files'])} ä¸ªæ–‡ä»¶")
        
        return result
    
    def get_current_files_info(self) -> dict:
        """è·å–å½“å‰æ–‡ä»¶ä¿¡æ¯"""
        info = {
            'data_files': [],
            'backup_files': [],
            'watch_files': [],
            'processed_files': []
        }
        
        try:
            # æ£€æŸ¥dataç›®å½•
            if os.path.exists(self.data_directory):
                for filename in os.listdir(self.data_directory):
                    if self.is_stock_data_file(filename):
                        filepath = os.path.join(self.data_directory, filename)
                        file_info = {
                            'name': filename,
                            'path': filepath,
                            'size': os.path.getsize(filepath),
                            'modified': datetime.fromtimestamp(os.path.getmtime(filepath)).strftime('%Y-%m-%d %H:%M:%S')
                        }
                        info['data_files'].append(file_info)
            
            # æ£€æŸ¥å¤‡ä»½ç›®å½•
            if os.path.exists(self.backup_directory):
                for filename in os.listdir(self.backup_directory):
                    if filename.endswith('.csv'):
                        filepath = os.path.join(self.backup_directory, filename)
                        file_info = {
                            'name': filename,
                            'path': filepath,
                            'size': os.path.getsize(filepath),
                            'modified': datetime.fromtimestamp(os.path.getmtime(filepath)).strftime('%Y-%m-%d %H:%M:%S')
                        }
                        info['backup_files'].append(file_info)
            
            # æ£€æŸ¥ç›‘æ§ç›®å½•
            for filename in os.listdir(self.watch_directory):
                if self.is_stock_data_file(filename):
                    filepath = os.path.join(self.watch_directory, filename)
                    file_info = {
                        'name': filename,
                        'path': filepath,
                        'size': os.path.getsize(filepath),
                        'modified': datetime.fromtimestamp(os.path.getmtime(filepath)).strftime('%Y-%m-%d %H:%M:%S')
                    }
                    info['watch_files'].append(file_info)

            # å·²å¤„ç†æ–‡ä»¶ä¿¡æ¯ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼Œä¸å†ä½¿ç”¨processedç›®å½•ï¼‰
            # å¯ä»¥æ˜¾ç¤ºå¤‡ä»½æ–‡ä»¶ä½œä¸ºå·²å¤„ç†çš„æ–‡ä»¶

        except Exception as e:
            logger.error(f"è·å–æ–‡ä»¶ä¿¡æ¯å¤±è´¥: {e}")

        return info


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ è‡ªåŠ¨æ–‡ä»¶ç®¡ç†å™¨")
    print("=" * 50)
    
    manager = AutoFileManager()
    
    # æ˜¾ç¤ºå½“å‰æ–‡ä»¶çŠ¶æ€
    files_info = manager.get_current_files_info()
    
    print(f"ğŸ“ æ•°æ®æ–‡ä»¶ ({len(files_info['data_files'])} ä¸ª):")
    for file_info in files_info['data_files']:
        size_mb = file_info['size'] / (1024 * 1024)
        print(f"  - {file_info['name']} ({size_mb:.2f} MB, {file_info['modified']})")
    
    print(f"\nğŸ“ ç›‘æ§ç›®å½•æ–‡ä»¶ ({len(files_info['watch_files'])} ä¸ª):")
    for file_info in files_info['watch_files']:
        size_mb = file_info['size'] / (1024 * 1024)
        print(f"  - {file_info['name']} ({size_mb:.2f} MB, {file_info['modified']})")

    print(f"\nğŸ“ å·²å¤„ç†æ–‡ä»¶ ({len(files_info['processed_files'])} ä¸ª):")
    for file_info in files_info['processed_files']:
        size_mb = file_info['size'] / (1024 * 1024)
        print(f"  - {file_info['name']} ({size_mb:.2f} MB, {file_info['modified']})")

    # æ‰§è¡Œè‡ªåŠ¨æ›´æ–°
    print(f"\nğŸ”„ æ‰§è¡Œè‡ªåŠ¨æ›´æ–°...")
    result = manager.auto_update()
    
    if result['updated']:
        print(f"âœ… æ›´æ–°æˆåŠŸï¼æ–°å®‰è£…æ–‡ä»¶: {result['new_files']}")
    else:
        print("â„¹ï¸  æ²¡æœ‰éœ€è¦æ›´æ–°çš„æ–‡ä»¶")
    
    if result['errors']:
        print(f"âŒ é”™è¯¯: {result['errors']}")


if __name__ == '__main__':
    main()
